# -*- coding: utf-8 -*-
"""Profissao_Analista_de_dados_M16_2_Exercicio_Feito.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13ln7ITlfG9aODM91zNHvDxA2IoI9Zddo

<img src="https://raw.githubusercontent.com/andre-marcos-perez/ebac-course-utils/main/media/logo/newebac_logo_black_half.png" alt="ebac-logo">

---

# **Módulo** | Análise de Dados: Análise Exploratória de Dados de Logística II
Caderno de **Exercícios**<br>
Professor [André Perez](https://www.linkedin.com/in/andremarcosperez/)

---

# **Tópicos**

<ol type="1">
  <li>Manipulação;</li>
  <li>Visualização;</li>
  <li>Storytelling.</li>
</ol>

---

# **Exercícios**

Este *notebook* deve servir como um guia para **você continuar** a construção da sua própria análise exploratória de dados. Fique a vontate para copiar os códigos da aula mas busque explorar os dados ao máximo. Por fim, publique seu *notebook* no [Kaggle](https://www.kaggle.com/).

---

# **Análise Exploratória de Dados de Logística**

## 1\. Contexto

Entender como funciona o comportamento e distribuição das entregas dentro do hub da loggi que possua mais entregas documentadas na base de dados

## 2\. Pacotes e bibliotecas
"""

# - 1º pacotes nativos do python: json, os, etc.;
import json

# - 2º pacotes de terceiros: pandas, seabornm etc.;
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import geopandas
import geopy
from geopy.geocoders import Nominatim
from geopy.extra.rate_limiter import RateLimiter

"""## 3\. Exploração de dados"""

# faça o código de exploração de dados:

# Primeiro fazemos o download do arquivo e colocamos num arquivo json com o nome de 'deliveries.json'
!wget -q "https://raw.githubusercontent.com/andre-marcos-perez/ebac-course-utils/main/dataset/deliveries.json" -O deliveries.json

# coleta de dados:
with open('deliveries.json', mode='r', encoding='utf8') as file:
  data = json.load(file)

# para entender o tamanho da base de dados:
len(data)

# - wrangling da estrutura:
deliveries_df = pd.DataFrame(data)
#deliveries_df.head() -> usei para poder visualizar os próximos passos, desabilitei pois não há necessidade de mostrar essa etapa processada, fiz o mesmo para cada etapa para poder visualizar os resultados.

# observando as colunas origin e deliveries, há a necessidade de explodí-las para poder ter uma base estruturada e começarmos a analisar a qualidade dos dados:

# tratando a coluna origin:

hub_origin_df = pd.json_normalize(deliveries_df["origin"])

deliveries_df = pd.merge(left=deliveries_df, right=hub_origin_df, how='inner', left_index=True, right_index=True)

deliveries_df = deliveries_df.drop("origin", axis=1)
deliveries_df = deliveries_df[["name", "region", "lng", "lat", "vehicle_capacity", "deliveries"]]

# foi observado que temos duas colunas para cada medição de latitude e longitude, portanto, optou-se por especificar cada campo relacionado a origem nesse caso:
deliveries_df.rename(columns={"lng": "hub_lng", "lat": "hub_lat"}, inplace=True)

# tratando a coluna deliveries:
deliveries_exploded_df = deliveries_df[["deliveries"]].explode("deliveries")

deliveries_normalized_df = pd.concat([
  pd.DataFrame(deliveries_exploded_df["deliveries"].apply(lambda record: record["size"])).rename(columns={"deliveries": "delivery_size"}),
  pd.DataFrame(deliveries_exploded_df["deliveries"].apply(lambda record: record["point"]["lng"])).rename(columns={"deliveries": "delivery_lng"}),
  pd.DataFrame(deliveries_exploded_df["deliveries"].apply(lambda record: record["point"]["lat"])).rename(columns={"deliveries": "delivery_lat"}),
], axis= 1)

# juntando os dados explodidos ao df principal:
deliveries_df = deliveries_df.drop("deliveries", axis=1)
deliveries_df = pd.merge(left=deliveries_df, right=deliveries_normalized_df, how='right', left_index=True, right_index=True)
deliveries_df.reset_index(inplace=True, drop=True)
deliveries_df.head()

# - exploração da estrutura:

# quantidade de linhas X colunas = (636149, 8)
deliveries_df.shape

# nomes das colunas =  Index(['name', 'region', 'hub_lng', 'hub_lat', 'vehicle_capacity','delivery_size', 'delivery_lng', 'delivery_lat'],dtype='object')
deliveries_df.columns

# range do índice
deliveries_df.index

# informação sobre os tipos das colunas e se existe algum valor nulo dentro delas
deliveries_df.info()

# - exploração do schema:

# colunas e tipos de dados:
deliveries_df.dtypes

# - exploração do schema:

# analisando colunas categóricas:
deliveries_df.select_dtypes("object").describe().transpose()

# - exploração do schema:

# analisando colunas numéricas (inteiros)
deliveries_df.drop(["name", "region"], axis=1).select_dtypes('int64').describe().transpose()

# - exploração do schema:

# analisando colunas numéricas (float)
deliveries_df.drop(["name", "region"], axis=1).select_dtypes('float64').describe().transpose()

# - exploração do schema:

# analisando se existem dados faltantes
deliveries_df.isna().any()

"""## 4\. Manipulação"""

# manipulação de dados:

# - enriquecimento:

# tratamento dos dados dos hubs:

# desmembrar a longitude e latitude para conseguir ter uma maior riqueza na análise dos dados
hub_df = deliveries_df[["region", "hub_lng", "hub_lat"]]
hub_df = hub_df.drop_duplicates().sort_values(by="region").reset_index(drop=True)

# localização reversa para conseguirmos informações mais detalhadas sobre o local do hub
geolocator = Nominatim(user_agent="ebac_geocoder")
geocoder = RateLimiter(geolocator.reverse, min_delay_seconds=1)

hub_df["coordinates"] = hub_df["hub_lat"].astype(str)  + ", " + hub_df["hub_lng"].astype(str)
hub_df["geodata"] = hub_df["coordinates"].apply(geocoder)

# como os dados ficaram juntos dentro de uma só célula, serão normalizados:
hub_geodata_df = pd.json_normalize(hub_df["geodata"].apply(lambda data: data.raw))

# selecionando somente as colunas que serão utilizadas nas análises e fazendo os tratamentos necessários:
hub_geodata_df = hub_geodata_df[["address.town", "address.suburb", "address.city"]]
hub_geodata_df.rename(columns={"address.town": "hub_town", "address.suburb": "hub_suburb", "address.city": "hub_city"}, inplace=True)
hub_geodata_df["hub_city"] = np.where(hub_geodata_df["hub_city"].notna(), hub_geodata_df["hub_city"], hub_geodata_df["hub_town"])
hub_geodata_df["hub_suburb"] = np.where(hub_geodata_df["hub_suburb"].notna(), hub_geodata_df["hub_suburb"], hub_geodata_df["hub_city"])
hub_geodata_df = hub_geodata_df.drop("hub_town", axis=1)

# combinando com o data frame principal
hub_df = pd.merge(left=hub_df, right=hub_geodata_df, left_index=True, right_index=True)
hub_df = hub_df[["region", "hub_suburb", "hub_city"]]

deliveries_df = pd.merge(left=deliveries_df, right=hub_df, how="inner", on="region")
deliveries_df = deliveries_df[["name", "region", "hub_lng", "hub_lat", "hub_city", "hub_suburb", "vehicle_capacity", "delivery_size", "delivery_lng", "delivery_lat"]]
deliveries_df.head()

# manipulação de dados:

# - enriquecimento:

# tratamento dos dados das entregas:

!wget -q "https://raw.githubusercontent.com/andre-marcos-perez/ebac-course-utils/main/dataset/deliveries-geodata.csv" -O deliveries-geodata.csv
deliveries_geodata_df = pd.read_csv("deliveries-geodata.csv")

# combinando com o data frame principal
deliveries_df = pd.merge(left=deliveries_df, right=deliveries_geodata_df[["delivery_city", "delivery_suburb"]], how="inner", left_index=True, right_index=True)
deliveries_df.head()

# - controle de qualidade:

# agora deve-se analisar novamente o schema para entender a qualidade dos dados depois de todos esses tratamentos:

# informação sobre os tipos das colunas e se existe algum valor nulo dentro delas
deliveries_df.info()

# - controle de qualidade:

# analisando se existem dados faltantes
deliveries_df.isna().any()

# - controle de qualidade:

# observando os campos delivery_city e delivery_suburb, pode-se perceber que possuem dados nulos. Portanto, o ideal é entender a porcentagem desses nulos.

na_delivery_city = 100 * (deliveries_df["delivery_city"].isna().sum() / len(deliveries_df))
print("% de na em delivery_city = ", na_delivery_city)

na_delivery_suburb = 100 * (deliveries_df["delivery_suburb"].isna().sum() / len(deliveries_df))
print(f'% de na em delivery_suburb = {na_delivery_suburb}')

# - controle de qualidade:

# proporção de entregas por cidade:
prop_df = deliveries_df[["delivery_city"]].value_counts() / len(deliveries_df)
prop_df.sort_values(ascending=False).head(10)

# - controle de qualidade:

# proporção de entregas por suburb:
prop_df = deliveries_df[["delivery_suburb"]].value_counts() / len(deliveries_df)
prop_df.sort_values(ascending=False).head(10)

"""## 5\. Visualização

Nesta etapa vamos entender um pouco sobre
"""

# faça o código de visualização de dados:
#
# - produza pelo menos duas visualizações;
# - adicione um pequeno texto com os insights encontrados;
# - etc.

# qual hub tem mais entregas?

resume_hub_df = deliveries_df.groupby("hub_city").size().reset_index(name='count').sort_values("count", ascending = False)

grafico = sns.barplot(data=resume_hub_df, x="hub_city", y="count", ci=None, palette="pastel")
grafico.set(title='Entregas por hub', xlabel='hub', ylabel='volume de entregas')

# agora é preciso filtrar o data frame principal para termos somente os dados do hub de Brasília (df-0), que é o que possui mais entregas:
bsa_deliveries_df = deliveries_df.query('hub_city == "Brasília"')
bsa_deliveries_df.head()

# agora vamos entender como que essas entregas estão distribuídas em volume pelo cidade de entrega:
sns.set(rc={'figure.figsize':(16,9)})
resume_suburb_df = bsa_deliveries_df.groupby("delivery_city").size().reset_index(name='count').sort_values("count", ascending = False).head(10)

grafico = sns.barplot(data=resume_suburb_df, x="delivery_city", y="count", ci=None, palette="pastel")
grafico.set(title='Entregas por cidade pelo hub de Brasília', xlabel='cidade de entrega', ylabel='volume de entregas')

"""A partir desse gráfico acima, pode-se observar que: mesmo tendo dois hubs, um em Sobradinho e outro em Taguatinga. O hub de Brasília têm entregas com destino para essas duas cidades, num volume considerável."""

# agora vamos entender como que essas entregas estão distribuídas olhando para o mapa com todos os hubs, para entender se faz sentido essas entregas em cidades que já possuem hub.
# Para isso, será necessário algumas etapas de coleta de dados.

# importação do mapa do df:
!wget -q "https://geoftp.ibge.gov.br/cartas_e_mapas/bases_cartograficas_continuas/bc100/go_df/versao2016/shapefile/bc100_go_df_shp.zip" -O distrito-federal.zip
!unzip -q distrito-federal.zip -d ./maps
!cp ./maps/LIM_Unidade_Federacao_A.shp ./distrito-federal.shp
!cp ./maps/LIM_Unidade_Federacao_A.shx ./distrito-federal.shx

mapa = geopandas.read_file("distrito-federal.shp")
mapa = mapa.loc[[0]]

# criando o data frame com a coluna nova com o ponto de identificação no mapa do hub
hub_df = deliveries_df[["region", "hub_lng", "hub_lat"]].drop_duplicates().reset_index(drop=True)
geo_hub_df = geopandas.GeoDataFrame(hub_df, geometry=geopandas.points_from_xy(hub_df["hub_lng"], hub_df["hub_lat"]))

# criando o data frame com a coluna nova com o ponto de identificação no mapa da entrega
geo_deliveries_df = geopandas.GeoDataFrame(deliveries_df, geometry=geopandas.points_from_xy(deliveries_df["delivery_lng"], deliveries_df["delivery_lat"]))

#agora o mesmo gráfico, mas somente para o hub de brasilia diferenciando a cidade:

# cria o plot vazio
fig, ax = plt.subplots(figsize = (50/2.54, 50/2.54))

# plot mapa do distrito federal
mapa.plot(ax=ax, alpha=0.4, color="lightgrey")

# plot das entregas
geo_deliveries_df.query("region == 'df-0'").plot(ax=ax, markersize=1, color="red", label="df-0")
geo_deliveries_df.query("region == 'df-1'").plot(ax=ax, markersize=1, color="blue", label="df-1")
geo_deliveries_df.query("region == 'df-2'").plot(ax=ax, markersize=1, color="seagreen", label="df-2")

# plot dos hubs
geo_hub_df.plot(ax=ax, markersize=30, marker="x", color="black", label="hub")

# plot da legenda
plt.title("Entregas no Distrito Federal por Região", fontdict={"fontsize": 16})
lgnd = plt.legend(prop={"size": 15})
for handle in lgnd.legendHandles:
    handle.set_sizes([50])

#agora o mesmo gráfico, mas somente para o hub de brasilia diferenciando a cidade:

# cria o plot vazio
fig, ax = plt.subplots(figsize = (50/2.54, 50/2.54))

# plot mapa do distrito federal
mapa.plot(ax=ax, alpha=0.4, color="lightgrey")

# plot das entregas
geo_deliveries_df.query("region == 'df-1'").query("delivery_city == 'Sobradinho'").plot(ax=ax, markersize=1, color="blue", label="Sobradinho")
geo_deliveries_df.query("region == 'df-1'").query("delivery_city == 'Taguatinga'").plot(ax=ax, markersize=1, color="seagreen", label="Taguatinga")
geo_deliveries_df.query("region == 'df-1'").query("delivery_city == 'Brasília'").plot(ax=ax, markersize=1, color="red", label="Brasília")

# plot dos hubs
geo_hub_df.plot(ax=ax, markersize=30, marker="x", color="black", label="hub")

# plot da legenda
plt.title("Entregas no hub de Brasília por Cidade da entrega", fontdict={"fontsize": 16})
lgnd = plt.legend(prop={"size": 15})
for handle in lgnd.legendHandles:
    handle.set_sizes([50])

"""A partir dessa análise pudemos identificar alguns casos esquisitos. Entregas muito próxima uma das outras estão cadastradas como cidades diferentes. O que gera uma certa confusão quando paramos para analisar. Pois bota em questionamento a qualidade da base de dados em questão."""
